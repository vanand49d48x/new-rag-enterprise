model:
  name: tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
  type: chat
  size: 1.1b
  context_size: 1024
  max_tokens: 128
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  available_models:
    tinyllama:
      name: tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
      size: 1.1b
      context_size: 2048
      max_memory: 4GB
      threads: 8
      batch_size: 256
    qwen25_3b:
      name: qwen2.5-3b-instruct-q4_k_m.gguf
      size: 3b
      context_size: 4096
      max_memory: 16GB
      threads: 8
      batch_size: 512
    qwen2_7b:
      name: qwen2-7b-instruct-q4_k_m.gguf
      size: 7b
      context_size: 4096
      max_memory: 24GB
      threads: 12
      batch_size: 1024
performance:
  threads: 8
  batch_size: 256
  gpu_layers: 0
  max_memory: 24GB
  memory_efficient: true
  gradient_checkpointing: true
  activation_checkpointing: true
  omp_num_threads: 12
  mkl_num_threads: 12
  malloc_arena_max: 2
  gomp_cpu_affinity: 0-11
  numa_aware: true
  cache_optimized: true
  prefetch_enabled: true
quantization:
  method: nf4
  bits: 4
  group_size: 128
  desc_act: true
  nf4_quant:
    enabled: true
    bnb_4bit_use_double_quant: true
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_quant_type: nf4
    memory_efficient: true
lora:
  enabled: true
  base_model: Qwen/Qwen2.5-3B-Instruct
  lora_model_path: ./models/lora/qwen25_lora
  r: 16
  lora_alpha: 32
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  learning_rate: 2e-4
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
moe:
  enabled: true
  num_experts: 8
  top_k: 2
  capacity_factor: 1.0
  aux_loss_weight: 0.01
  router_type: standard
  router_jitter: 0.0
  router_aux_loss_coef: 0.001
advanced:
  use_flash_attention: true
  use_cache: true
  torch_dtype: bfloat16
  use_memory_efficient_attention: true
  parallel_processing: true
  async_processing: true
  stream_processing: true
  enable_work_stealing: true
  thread_pool_size: 12
deployment:
  platform: linux/amd64
  restart_policy: unless-stopped
  backend_port: 8000
  llm_port: 8080
  qdrant_port: 6334
  grafana_port: 3000
  prometheus_port: 9090
  health_check_interval: 30s
  health_check_timeout: 10s
  health_check_retries: 3
monitoring:
  enabled: true
  prometheus: true
  grafana: true
  collect_performance_metrics: true
  collect_memory_metrics: true
  collect_cpu_metrics: true
  log_level: INFO
  log_format: json
