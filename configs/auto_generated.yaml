# Auto-generated configuration for enterprise tier
# Generated on: Thu Jul 31 22:23:59 EDT 2025
# System: 12 cores, 32GB RAM, GPU: Not available

llm:
  model: qwen2.5-3b-instruct-q4_k_m.gguf
  backend: llama_cpp
  api_url: http://llama-cpp:8080/completion
  timeout: 60
  max_tokens: 256
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  context_size: 4096
  batch_size: 256
  threads: 8
  gpu_layers: 0
  stream: true
  flash_attention: false

processing:
  chunk_size: 512
  chunk_overlap: 128
  max_workers: 6
  embedding_batch_size: 32
  prompt_trimming: true
  max_prompt_tokens: 500
  max_chunks: 3

api:
  timeout: 60
  stream_responses: true
  enable_flash_attention: false

vector_store:
  max_results: 5
  max_chunks_per_query: 3
  similarity_threshold: 0.7

system:
  tier: enterprise
  cpu_cores: 12
  ram_gb: 32
  gpu_available: false
