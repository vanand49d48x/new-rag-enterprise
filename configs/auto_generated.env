# Auto-generated from config.yaml
MODEL_NAME=qwen2.5-3b-instruct-q4_k_m.gguf
LLAMA_IMAGE=ghcr.io/ggerganov/llama.cpp:server
THREADS=12
BATCH_SIZE=512
CTX_SIZE=4096
GPU_LAYERS=0
N_PREDICT=256
REPEAT_PENALTY=1.1
TEMP=0.7
TOP_P=0.9
TOP_K=40

# System optimizations
OMP_NUM_THREADS=12
MKL_NUM_THREADS=12
OPENBLAS_NUM_THREADS=12
MALLOC_ARENA_MAX=2
