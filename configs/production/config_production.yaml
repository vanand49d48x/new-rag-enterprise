# Optimized RAG System Configuration
# Keeps Qwen2-7B but applies performance optimizations

# Model Configuration - Keep Qwen2-7B but optimize settings
model:
  name: "qwen2.5-3b"  # Keep the powerful model
  # Model-specific settings optimized for speed
  qwen2-7b:
    filename: "qwen2.5-3b"
    context_size: 2048
    threads: 16  # Increased from 8 to 16 for better performance
    batch_size: 512
    max_tokens: 128
  qwen2.5-3b:
    filename: "qwen2.5-3b"
    context_size: 2048
    threads: 16
    batch_size: 512
    max_tokens: 128
  tinyllama:
    filename: "qwen2.5-3b"
    context_size: 1024
    threads: 16
    batch_size: 512
    max_tokens: 128

# API Configuration - Optimized timeouts
api:
  host: "0.0.0.0"
  port: 8000
  timeout: 120  # Reduced from 180 to 120 seconds
  max_file_size: 100

# Vector Store Configuration
vector_store:
  host: "qdrant"
  port: 6334
  collection_name: "qwen2.5-3b"
  vector_size: 384

# LLM Configuration - Optimized for speed while keeping quality
llm:
  backend: "llama_cpp"
  api_url: "http://llama-cpp:8080/completion"
  temperature: 0.6  # Slightly reduced from 0.7 for faster, more focused responses
  top_p: 0.85  # Reduced from 0.9 for faster generation
  top_k: 30  # Reduced from 40 for faster generation
  repeat_penalty: 1.08  # Slightly reduced for speed

# Processing Configuration - Optimized for speed
processing:
  chunk_size: 350  # Slightly reduced from 400 for faster processing
  chunk_overlap: 40  # Reduced from 50
  max_workers: 6  # Increased from 4 to 6 for better parallel processing

# Monitoring Configuration
monitoring:
  prometheus_port: 9090
  grafana_port: 3000
  grafana_password: "admin"

# Deployment Configuration
deployment:
  platform: "auto"
  restart_policy: "unless-stopped"
  health_check_interval: 30
  health_check_timeout: 10
  health_check_retries: 3 