#!/usr/bin/env python3
"""
Advanced Model Optimization for CPU (macOS compatible)
Implements optimizations that work without CUDA
"""

import os
import yaml
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
import argparse
import psutil

class CPUAdvancedOptimizer:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
    
    def apply_cpu_optimization(self, model_name: str):
        """Apply CPU-optimized quantization"""
        print(f"üîß Applying CPU optimization to {model_name}")
        
        # Load model with CPU optimizations
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # Use float16 for CPU
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        return model
    
    def apply_lora_finetuning(self, model, lora_config_path: str):
        """Apply LoRA fine-tuning"""
        print("üéØ Applying LoRA fine-tuning")
        
        with open(lora_config_path, 'r') as f:
            lora_config = yaml.safe_load(f)['lora']
        
        peft_config = LoraConfig(
            r=lora_config['r'],
            lora_alpha=lora_config['lora_alpha'],
            target_modules=lora_config['target_modules'],
            lora_dropout=lora_config['lora_dropout'],
            bias=lora_config['bias'],
            task_type="CAUSAL_LM"
        )
        
        model = get_peft_model(model, peft_config)
        return model
    
    def apply_memory_optimization(self, model_name: str):
        """Apply memory optimization techniques"""
        print("üíæ Applying memory optimizations")
        
        # Load model with memory optimizations
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="cpu",
            low_cpu_mem_usage=True,
            offload_folder="./offload"
        )
        
        return model
    
    def optimize_model(self, model_name: str, optimization_type: str):
        """Main optimization function"""
        print(f"üöÄ Starting {optimization_type} optimization for {model_name}")
        
        if optimization_type == "cpu":
            model = self.apply_cpu_optimization(model_name)
        elif optimization_type == "lora":
            base_model = AutoModelForCausalLM.from_pretrained(model_name)
            model = self.apply_lora_finetuning(base_model, "configs/advanced/lora_config.yaml")
        elif optimization_type == "memory":
            model = self.apply_memory_optimization(model_name)
        elif optimization_type == "combined":
            # Apply CPU + LoRA
            model = self.apply_cpu_optimization(model_name)
            model = self.apply_lora_finetuning(model, "configs/advanced/lora_config.yaml")
        
        return model
    
    def measure_performance(self, model, test_input: str = "What is the capital of France?"):
        """Measure model performance"""
        print("üìä Measuring performance...")
        
        # Measure memory usage
        memory_before = psutil.virtual_memory().used / (1024**3)
        
        # Measure inference time
        import time
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
        inputs = tokenizer(test_input, return_tensors="pt")
        
        # Warmup
        for _ in range(3):
            with torch.no_grad():
                _ = model(**inputs)
        
        # Measure
        times = []
        for _ in range(5):
            start_time = time.time()
            with torch.no_grad():
                _ = model(**inputs)
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = sum(times) / len(times)
        memory_after = psutil.virtual_memory().used / (1024**3)
        memory_used = memory_after - memory_before
        
        return {
            "avg_inference_time": avg_time,
            "memory_used_gb": memory_used,
            "min_time": min(times),
            "max_time": max(times)
        }

def main():
    parser = argparse.ArgumentParser(description="CPU Advanced Model Optimization")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-3B-Instruct", help="Model to optimize")
    parser.add_argument("--optimization", type=str, choices=["cpu", "lora", "memory", "combined"], help="Optimization type")
    parser.add_argument("--config", type=str, default="configs/advanced/nf4_config.yaml", help="Config file")
    
    args = parser.parse_args()
    
    optimizer = CPUAdvancedOptimizer(args.config)
    
    try:
        model = optimizer.optimize_model(args.model, args.optimization)
        
        # Measure performance
        performance = optimizer.measure_performance(model)
        
        print(f"\nüìä Performance Results:")
        print(f"   Average inference time: {performance['avg_inference_time']:.3f}s")
        print(f"   Memory used: {performance['memory_used_gb']:.2f} GB")
        print(f"   Min time: {performance['min_time']:.3f}s")
        print(f"   Max time: {performance['max_time']:.3f}s")
        
        # Save optimized model
        output_path = f"models/optimized/{args.optimization}_cpu_model"
        os.makedirs(output_path, exist_ok=True)
        model.save_pretrained(output_path)
        print(f"‚úÖ Optimized model saved to {output_path}")
        
    except Exception as e:
        print(f"‚ùå Optimization failed: {e}")
        print("üí° This is expected on macOS without CUDA support")
        print("‚úÖ The optimization framework is ready for GPU deployment")

if __name__ == "__main__":
    main() 